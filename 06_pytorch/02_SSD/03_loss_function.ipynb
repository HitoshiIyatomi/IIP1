{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03_loss_function.ipynb\n",
    "\n",
    "Implement loss functions of SSD.\n",
    "including Hard Negative Mining.\n",
    "\n",
    "###  This uses utils.match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package and functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils.match import match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement loss function - MultiBoxLoss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"loss function of SSD\"\"\"\n",
    "\n",
    "    def __init__(self, jaccard_thresh=0.5, neg_pos=3, device='cpu'):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.jaccard_thresh = jaccard_thresh  # 0.5 : threshold of jaccard index in function the match\n",
    "        self.negpos_ratio = neg_pos  # 3:1 : ratio of Hard Negative Mining neg:pos\n",
    "        self.device = device  # CPU or GPU\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        calculation of loss function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        predictions : output of SSD network in the training (tuple)\n",
    "            (loc=torch.Size([num_batch, 8732, 4]), conf=torch.Size([num_batch, 8732, 21]), dbox_list=torch.Size [8732,4])。\n",
    "\n",
    "        targets : [num_batch, num_objs, 5]\n",
    "            5 is GT annotation : [xmin, ymin, xmax, ymax, label_ind]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss_l : Tensor\n",
    "            loss of loc\n",
    "        loss_c : Tensor\n",
    "            loss of conf\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Disjoint the output of tuppled SSD model\n",
    "        loc_data, conf_data, dbox_list = predictions\n",
    "\n",
    "        # number of elements\n",
    "        num_batch = loc_data.size(0)    # mini-batch size\n",
    "        num_dbox = loc_data.size(1)     # num of DBox= 8732\n",
    "        num_classes = conf_data.size(2) # num of classes = 21\n",
    "\n",
    "        # create tentative variables for loss calculation \n",
    "        # conf_t_label：Store the label of the nearest correct BBox for each DBox\n",
    "        # loc_t:Store the location of the nearest correct BBox for each DBox\n",
    "        conf_t_label = torch.LongTensor(num_batch, num_dbox).to(self.device)\n",
    "        loc_t = torch.Tensor(num_batch, num_dbox, 4).to(self.device)\n",
    "\n",
    "        # Override the results of match of DBox and 'targets' (correct annotation target)\n",
    "        #  on loc_t and conf_t_label\n",
    "        for idx in range(num_batch):  # mini-batch loop\n",
    "\n",
    "            # Get correct annotation of BBox and label in the current mini-batch\n",
    "            truths = targets[idx][:, :-1].to(self.device)  # BBox\n",
    "            # labels [label of obj1, label of obj2, …]\n",
    "            labels = targets[idx][:, -1].to(self.device)\n",
    "\n",
    "            # prepare a new variable for default box\n",
    "            dbox = dbox_list.to(self.device)\n",
    "\n",
    "            # Execute function \"match\" and update the contents of loc_t and conf_t_label\n",
    "            # （detail） For each BBox:\n",
    "            # loc_t: Overwrite it to the location of the nearest correct BBox\n",
    "            # conf_t_label：Overwrite it to the label of the nearest correct BBox\n",
    "            # However, if the jaccard overlap with the nearest BBox is less than 0.5,\n",
    "            #  the correct answer BBox label conf_t_label is 0 (the background class)\n",
    "  \n",
    "            variance = [0.1, 0.2]\n",
    "            # This variance is used to calculate the correction from DBox to BBox\n",
    "            match(self.jaccard_thresh, truths, dbox,\n",
    "                  variance, labels, loc_t, conf_t_label, idx)\n",
    "\n",
    "        # ----------\n",
    "        # Calculation of location-related loss (loss_l) with Smooth L1\n",
    "        #  (only for the offset of the DBox that found the object)\n",
    "        # ----------\n",
    "        # Mask to extract BBox from an object\n",
    "        pos_mask = conf_t_label > 0  # torch.Size([num_batch, 8732])\n",
    "\n",
    "        # reshape pos_mask to that of loc_data\n",
    "        pos_idx = pos_mask.unsqueeze(pos_mask.dim()).expand_as(loc_data)\n",
    "\n",
    "        # get loc_data and loc_t (training data) of Positive DBox\n",
    "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
    "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
    "\n",
    "        # Compute the loss of offset information (loc_t) for the Positive DBox that found the object\n",
    "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\n",
    "\n",
    "        # ----------\n",
    "        # Calculation of class prediction loss (loss_c) with cross-entropy\n",
    "        #\n",
    "        # Here, since there are overwhelming number of DBoxes with the correct background class, \n",
    "        # we perform Hard Negative Mining (HNM) so that the ratio of positive DBOX to background DBOX is 1:3.\n",
    "        # Furthermore, DBoxes that are predicted as background and whose losses are small, \n",
    "        #     are excluded from the calculation of class prediction loss (loss_c)   \n",
    "        # ----------\n",
    "        batch_conf = conf_data.view(-1, num_classes)\n",
    "\n",
    "        # calc for class loss\n",
    "        loss_c = F.cross_entropy(\n",
    "            batch_conf, conf_t_label.view(-1), reduction='none')\n",
    "\n",
    "        # -----------------\n",
    "        # Select negative-DBox used in Hard negative mining (HNM)\n",
    "        # -----------------\n",
    "\n",
    "        # Set the loss_c=0 if object is found\n",
    "        #  (note) object has an label >0, background label = 0 \n",
    "        num_pos = pos_mask.long().sum(1, keepdim=True)  # num of obj class in the min-batch\n",
    "        loss_c = loss_c.view(num_batch, -1)  # torch.Size([num_batch, 8732])\n",
    "        loss_c[pos_mask] = 0  # Set the loss_c=0 if object is found\n",
    "\n",
    "        # perform Hard Negative Mining (HNM)\n",
    "        # Find idx_rank, which is the rank of loss_c of each DBox\n",
    "        _, loss_idx = loss_c.sort(1, descending=True)\n",
    "        _, idx_rank = loss_idx.sort(1)\n",
    "\n",
    "        # get the number of background DBOX, 'num_neg'\n",
    "        # According to HNM, this number is three times larger (self.negpos_ratio) than that of positive DBox.\n",
    "        # If this is more than the total number of DBOXs, set it to this value.\n",
    "        num_neg = torch.clamp(num_pos*self.negpos_ratio, max=num_dbox)\n",
    "\n",
    "        # idx_rank contains the ranking of each DBox loss\n",
    "        # Select negative-Dbox with large recognition loss (recognized far from background) \n",
    "        # torch.Size([num_batch, 8732])\n",
    "        neg_mask = idx_rank < (num_neg).expand_as(idx_rank)\n",
    "\n",
    "        # -----------------\n",
    "        # （finish - selecting negative-Dbox use in HNM)\n",
    "        # -----------------\n",
    "\n",
    "        # reshape mask to meet the dimension of 'conf_data'\n",
    "        # pos_idx_mask: mask for Positive DBox (conf)\n",
    "        # neg_idx_mask: mask for Negative DBox selected in HNM (conf)\n",
    "        # pos_mask：torch.Size([num_batch, 8732])→pos_idx_mask：torch.Size([num_batch, 8732, 21])\n",
    "        pos_idx_mask = pos_mask.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx_mask = neg_mask.unsqueeze(2).expand_as(conf_data)\n",
    "\n",
    "        # concatenate positiveDB + negative-DB(selected HNM) formed conf_hnm\n",
    "        # the size of conf_hnm: torch.Size([num_pos+num_neg, 21])\n",
    "        conf_hnm = conf_data[(pos_idx_mask+neg_idx_mask).gt(0)].view(-1, num_classes)\n",
    "        # (note) gt: greater than (>) : this extracts index whose mask=1\n",
    "\n",
    "        # same as above, create conf_t_label_hnm: torch.Size([pos+neg])\n",
    "        conf_t_label_hnm = conf_t_label[(pos_mask+neg_mask).gt(0)]\n",
    "\n",
    "        # calculate loss for confidence (recognition)\n",
    "        loss_c = F.cross_entropy(conf_hnm, conf_t_label_hnm, reduction='sum')\n",
    "\n",
    "        # calculate loss_l and loss_c (divided by number of positive DBox; N)\n",
    "        N = num_pos.sum()\n",
    "        loss_l /= N\n",
    "        loss_c /= N\n",
    "\n",
    "        return loss_l, loss_c\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
